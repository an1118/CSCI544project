{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":18065,"status":"ok","timestamp":1682459506797,"user":{"displayName":"Li An","userId":"00679100983466103760"},"user_tz":420},"id":"mw8wYYjZMu4N","outputId":"953eda7f-25be-4543-89f7-9b4b5a29f7a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.15.0-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting linformer\n","  Downloading linformer-0.2.1-py3-none-any.whl (6.1 kB)\n","Collecting performer_pytorch\n","  Downloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n","Collecting GitPython!=3.1.29,>=1.0.0\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.21.0-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from linformer) (2.0.0+cu118)\n","Collecting local-attention>=1.1.1\n","  Downloading local_attention-1.8.5-py3-none-any.whl (8.1 kB)\n","Collecting einops>=0.3\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting axial-positional-embedding>=0.1.0\n","  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->linformer) (3.11.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->linformer) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->linformer) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->linformer) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->linformer) (3.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->linformer) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->linformer) (16.0.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->linformer) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->linformer) (1.3.0)\n","Building wheels for collected packages: axial-positional-embedding, pathtools\n","  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2901 sha256=a9a7a7544d703b9443795fc15118731a6ad453d21c03d2611a0e6519e076f0ad\n","  Stored in directory: /root/.cache/pip/wheels/e9/e0/51/fec72c3ac576d0559b7b3a328ec5dcbac4120cca74be9e49fc\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=9db3cf22ac66b238a818c73cb16959036d21a1ee1f50e1de28781802ebf9d85f\n","  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n","Successfully built axial-positional-embedding pathtools\n","Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, einops, docker-pycreds, gitdb, GitPython, wandb, local-attention, axial-positional-embedding, performer_pytorch, linformer\n","Successfully installed GitPython-3.1.31 axial-positional-embedding-0.2.1 docker-pycreds-0.4.0 einops-0.6.1 gitdb-4.0.10 linformer-0.2.1 local-attention-1.8.5 pathtools-0.1.2 performer_pytorch-1.1.4 sentry-sdk-1.21.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.0\n"]}],"source":["!pip3 install wandb linformer performer_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1446,"status":"ok","timestamp":1682460278350,"user":{"displayName":"Li An","userId":"00679100983466103760"},"user_tz":420},"id":"3Pu7Fp2Cckh2","outputId":"6cefb6e3-46fe-43fa-a0c1-9cd346c9f4e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/CSCI544/project/CDIL-CNN-main/experiments/_2lra\n","backup_lra_main.py  lra_config.py  lra_main1.py  lra_train.py\n","create_datasets     lra_datasets   lra_main.py\t __pycache__\n","lra_all.sh\t    lra_log\t   lra_model\n"]}],"source":["%cd /content/drive/MyDrive/CSCI544/project/CDIL-CNN-main/experiments/_2lra\n","# %cd /content/drive/MyDrive/CDIL-CNN-main/experiments/_2lra\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24546,"status":"ok","timestamp":1682459535305,"user":{"displayName":"Li An","userId":"00679100983466103760"},"user_tz":420},"id":"-QsWbotBNJDp","outputId":"e3a88f25-bd07-4109-b1f8-ceb2110e2a94"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131,"status":"ok","timestamp":1682225498319,"user":{"displayName":"Li An","userId":"00679100983466103760"},"user_tz":420},"id":"JqI-pcEkc_U4","outputId":"0fb9ff96-687a-4a5c-bc0a-0d18a522fee4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/CSCI544/project/CDIL-CNN-main/experiments/_2lra/create_datasets\n"]}],"source":["%cd create_datasets/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":164113,"status":"ok","timestamp":1682226731372,"user":{"displayName":"Li An","userId":"00679100983466103760"},"user_tz":420},"id":"GXO2dD7Ug-rg","outputId":"e43ef941-2d00-4ca3-db97-368a9a76f4d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-04-23 05:09:28.988447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-23 05:09:30.417229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n","2023-04-23 05:09:33.627340: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n","Dl Completed...: 0 url [00:00, ? url/s]\n","Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n","Dl Size...:   0% 0/80 [00:00<?, ? MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:01<?, ? url/s]\n","Dl Size...:   1% 1/80 [00:01<01:23,  1.06s/ MiB]\u001b[A\n","Dl Completed...:   0% 0/1 [00:01<?, ? url/s]\n","Dl Size...:   2% 2/80 [00:01<00:49,  1.58 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:01<?, ? url/s]\n","Dl Size...:   4% 3/80 [00:01<00:34,  2.21 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:01<?, ? url/s]\n","Dl Size...:   5% 4/80 [00:01<00:25,  3.02 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:01<?, ? url/s]\n","Dl Size...:   6% 5/80 [00:01<00:19,  3.77 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Size...:   9% 7/80 [00:02<00:16,  4.50 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Size...:  11% 9/80 [00:02<00:11,  6.25 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Size...:  14% 11/80 [00:02<00:08,  8.08 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Size...:  16% 13/80 [00:02<00:06, 10.36 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Size...:  20% 16/80 [00:02<00:05, 11.55 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Size...:  24% 19/80 [00:02<00:03, 15.54 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Size...:  29% 23/80 [00:02<00:03, 17.50 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  34% 27/80 [00:03<00:02, 22.30 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  40% 32/80 [00:03<00:01, 24.90 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  46% 37/80 [00:03<00:01, 30.06 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  52% 42/80 [00:03<00:01, 34.73 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  60% 48/80 [00:03<00:00, 37.49 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  68% 54/80 [00:03<00:00, 42.35 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  75% 60/80 [00:03<00:00, 46.04 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  82% 66/80 [00:03<00:00, 48.50 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:03<?, ? url/s]\n","Dl Size...:  90% 72/80 [00:03<00:00, 50.07 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Size...:  98% 78/80 [00:04<00:00, 51.19 MiB/s]\u001b[A\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Completed...:   0% 0/1 [00:04<?, ? url/s]\n","Dl Completed...: 100% 1/1 [00:04<00:00,  4.13s/ url]\n","Dl Size...: 100% 80/80 [00:04<00:00, 19.36 MiB/s]\n","Dl Completed...: 100% 1/1 [00:04<00:00,  4.13s/ url]\n","Generating splits...:   0% 0/3 [00:00<?, ? splits/s]\n","Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n","Generating train examples...: 1 examples [00:03,  3.48s/ examples]\u001b[A\n","Generating train examples...: 519 examples [00:03, 204.56 examples/s]\u001b[A\n","Generating train examples...: 871 examples [00:03, 381.37 examples/s]\u001b[A\n","Generating train examples...: 1339 examples [00:03, 685.66 examples/s]\u001b[A\n","Generating train examples...: 1826 examples [00:03, 1069.81 examples/s]\u001b[A\n","Generating train examples...: 2313 examples [00:03, 1510.45 examples/s]\u001b[A\n","Generating train examples...: 2770 examples [00:04, 1945.00 examples/s]\u001b[A\n","Generating train examples...: 3233 examples [00:04, 2397.71 examples/s]\u001b[A\n","Generating train examples...: 3727 examples [00:04, 2889.78 examples/s]\u001b[A\n","Generating train examples...: 4192 examples [00:04, 2292.07 examples/s]\u001b[A\n","Generating train examples...: 4658 examples [00:04, 2713.49 examples/s]\u001b[A\n","Generating train examples...: 5142 examples [00:04, 3144.29 examples/s]\u001b[A\n","Generating train examples...: 5603 examples [00:04, 3473.14 examples/s]\u001b[A\n","Generating train examples...: 6056 examples [00:04, 3729.24 examples/s]\u001b[A\n","Generating train examples...: 6497 examples [00:05, 3807.01 examples/s]\u001b[A\n","Generating train examples...: 6956 examples [00:05, 4010.94 examples/s]\u001b[A\n","Generating train examples...: 7427 examples [00:05, 4200.59 examples/s]\u001b[A\n","Generating train examples...: 7875 examples [00:05, 4196.83 examples/s]\u001b[A\n","Generating train examples...: 8321 examples [00:05, 4267.99 examples/s]\u001b[A\n","Generating train examples...: 8774 examples [00:05, 4342.52 examples/s]\u001b[A\n","Generating train examples...: 9232 examples [00:05, 4410.33 examples/s]\u001b[A\n","Generating train examples...: 9698 examples [00:05, 4483.04 examples/s]\u001b[A\n","Generating train examples...: 10152 examples [00:05, 4497.91 examples/s]\u001b[A\n","Generating train examples...: 10627 examples [00:05, 4571.18 examples/s]\u001b[A\n","Generating train examples...: 11087 examples [00:06, 4556.35 examples/s]\u001b[A\n","Generating train examples...: 11548 examples [00:06, 4568.71 examples/s]\u001b[A\n","Generating train examples...: 12028 examples [00:06, 4635.78 examples/s]\u001b[A\n","Generating train examples...: 12493 examples [00:06, 4477.25 examples/s]\u001b[A\n","Generating train examples...: 12943 examples [00:06, 4462.24 examples/s]\u001b[A\n","Generating train examples...: 13421 examples [00:06, 4552.09 examples/s]\u001b[A\n","Generating train examples...: 13878 examples [00:06, 4496.16 examples/s]\u001b[A\n","Generating train examples...: 14349 examples [00:06, 4556.04 examples/s]\u001b[A\n","Generating train examples...: 14816 examples [00:06, 4588.09 examples/s]\u001b[A\n","Generating train examples...: 15302 examples [00:07, 4667.75 examples/s]\u001b[A\n","Generating train examples...: 15770 examples [00:07, 4595.38 examples/s]\u001b[A\n","Generating train examples...: 16231 examples [00:07, 4571.41 examples/s]\u001b[A\n","Generating train examples...: 16703 examples [00:07, 4612.51 examples/s]\u001b[A\n","Generating train examples...: 17165 examples [00:07, 4500.68 examples/s]\u001b[A\n","Generating train examples...: 17630 examples [00:07, 4543.41 examples/s]\u001b[A\n","Generating train examples...: 18085 examples [00:07, 4499.94 examples/s]\u001b[A\n","Generating train examples...: 18536 examples [00:07, 4500.60 examples/s]\u001b[A\n","Generating train examples...: 18997 examples [00:07, 4531.33 examples/s]\u001b[A\n","Generating train examples...: 19451 examples [00:07, 4503.96 examples/s]\u001b[A\n","Generating train examples...: 19903 examples [00:08, 4506.58 examples/s]\u001b[A\n","Generating train examples...: 20360 examples [00:08, 4525.24 examples/s]\u001b[A\n","Generating train examples...: 20813 examples [00:08, 4499.35 examples/s]\u001b[A\n","Generating train examples...: 21275 examples [00:08, 4533.95 examples/s]\u001b[A\n","Generating train examples...: 21729 examples [00:08, 4423.14 examples/s]\u001b[A\n","Generating train examples...: 22172 examples [00:08, 4414.07 examples/s]\u001b[A\n","Generating train examples...: 22625 examples [00:08, 4446.20 examples/s]\u001b[A\n","Generating train examples...: 23070 examples [00:08, 4202.40 examples/s]\u001b[A\n","Generating train examples...: 23494 examples [00:08, 3903.51 examples/s]\u001b[A\n","Generating train examples...: 23933 examples [00:08, 4036.12 examples/s]\u001b[A\n","Generating train examples...: 24394 examples [00:09, 4196.86 examples/s]\u001b[A\n","Generating train examples...: 24853 examples [00:09, 4306.48 examples/s]\u001b[A\n","                                                                        \u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-train.tfrecord*...:   0% 0/25000 [00:00<?, ? examples/s]\u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-train.tfrecord*...:  52% 12986/25000 [00:00<00:00, 129844.03 examples/s]\u001b[A\n","Generating splits...:  33% 1/3 [00:13<00:26, 13.14s/ splits]\n","Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n","Generating test examples...: 1 examples [00:00,  1.19 examples/s]\u001b[A\n","Generating test examples...: 327 examples [00:00, 474.64 examples/s]\u001b[A\n","Generating test examples...: 679 examples [00:01, 998.04 examples/s]\u001b[A\n","Generating test examples...: 1015 examples [00:01, 1463.55 examples/s]\u001b[A\n","Generating test examples...: 1328 examples [00:01, 1829.41 examples/s]\u001b[A\n","Generating test examples...: 1649 examples [00:01, 2158.44 examples/s]\u001b[A\n","Generating test examples...: 1997 examples [00:01, 2494.11 examples/s]\u001b[A\n","Generating test examples...: 2318 examples [00:01, 2600.31 examples/s]\u001b[A\n","Generating test examples...: 2639 examples [00:01, 2764.38 examples/s]\u001b[A\n","Generating test examples...: 2953 examples [00:01, 2841.50 examples/s]\u001b[A\n","Generating test examples...: 3264 examples [00:01, 2907.12 examples/s]\u001b[A\n","Generating test examples...: 3574 examples [00:01, 2908.79 examples/s]\u001b[A\n","Generating test examples...: 3878 examples [00:02, 2936.37 examples/s]\u001b[A\n","Generating test examples...: 4224 examples [00:02, 3086.28 examples/s]\u001b[A\n","Generating test examples...: 4560 examples [00:02, 3162.88 examples/s]\u001b[A\n","Generating test examples...: 4883 examples [00:02, 3180.80 examples/s]\u001b[A\n","Generating test examples...: 5215 examples [00:02, 3219.20 examples/s]\u001b[A\n","Generating test examples...: 5540 examples [00:02, 3035.42 examples/s]\u001b[A\n","Generating test examples...: 5848 examples [00:02, 3038.67 examples/s]\u001b[A\n","Generating test examples...: 6176 examples [00:02, 3105.84 examples/s]\u001b[A\n","Generating test examples...: 6489 examples [00:02, 3092.27 examples/s]\u001b[A\n","Generating test examples...: 6830 examples [00:02, 3184.02 examples/s]\u001b[A\n","Generating test examples...: 7172 examples [00:03, 3250.46 examples/s]\u001b[A\n","Generating test examples...: 7499 examples [00:03, 3229.02 examples/s]\u001b[A\n","Generating test examples...: 7823 examples [00:03, 3225.48 examples/s]\u001b[A\n","Generating test examples...: 8164 examples [00:03, 3278.87 examples/s]\u001b[A\n","Generating test examples...: 8501 examples [00:03, 3302.67 examples/s]\u001b[A\n","Generating test examples...: 8832 examples [00:03, 3256.81 examples/s]\u001b[A\n","Generating test examples...: 9159 examples [00:03, 3166.68 examples/s]\u001b[A\n","Generating test examples...: 9477 examples [00:03, 3095.13 examples/s]\u001b[A\n","Generating test examples...: 9802 examples [00:03, 3139.44 examples/s]\u001b[A\n","Generating test examples...: 10122 examples [00:04, 3155.05 examples/s]\u001b[A\n","Generating test examples...: 10446 examples [00:04, 3179.27 examples/s]\u001b[A\n","Generating test examples...: 10765 examples [00:04, 3155.03 examples/s]\u001b[A\n","Generating test examples...: 11081 examples [00:04, 3004.57 examples/s]\u001b[A\n","Generating test examples...: 11562 examples [00:04, 3520.10 examples/s]\u001b[A\n","Generating test examples...: 12042 examples [00:04, 3889.52 examples/s]\u001b[A\n","Generating test examples...: 12530 examples [00:04, 4179.08 examples/s]\u001b[A\n","Generating test examples...: 12952 examples [00:04, 4131.39 examples/s]\u001b[A\n","Generating test examples...: 13468 examples [00:04, 4430.63 examples/s]\u001b[A\n","Generating test examples...: 13916 examples [00:04, 4442.44 examples/s]\u001b[A\n","Generating test examples...: 14388 examples [00:05, 4519.98 examples/s]\u001b[A\n","Generating test examples...: 14892 examples [00:05, 4671.67 examples/s]\u001b[A\n","Generating test examples...: 15365 examples [00:05, 4688.85 examples/s]\u001b[A\n","Generating test examples...: 15854 examples [00:05, 4747.12 examples/s]\u001b[A\n","Generating test examples...: 16349 examples [00:05, 4806.58 examples/s]\u001b[A\n","Generating test examples...: 16831 examples [00:05, 4687.39 examples/s]\u001b[A\n","Generating test examples...: 17317 examples [00:05, 4735.84 examples/s]\u001b[A\n","Generating test examples...: 17792 examples [00:05, 4658.18 examples/s]\u001b[A\n","Generating test examples...: 18274 examples [00:05, 4703.94 examples/s]\u001b[A\n","Generating test examples...: 18762 examples [00:05, 4755.43 examples/s]\u001b[A\n","Generating test examples...: 19276 examples [00:06, 4867.34 examples/s]\u001b[A\n","Generating test examples...: 19770 examples [00:06, 4886.43 examples/s]\u001b[A\n","Generating test examples...: 20267 examples [00:06, 4909.13 examples/s]\u001b[A\n","Generating test examples...: 20759 examples [00:06, 4865.33 examples/s]\u001b[A\n","Generating test examples...: 21246 examples [00:06, 4830.18 examples/s]\u001b[A\n","Generating test examples...: 21730 examples [00:06, 4770.09 examples/s]\u001b[A\n","Generating test examples...: 22211 examples [00:06, 4779.38 examples/s]\u001b[A\n","Generating test examples...: 22690 examples [00:06, 4538.24 examples/s]\u001b[A\n","Generating test examples...: 23147 examples [00:06, 4309.94 examples/s]\u001b[A\n","Generating test examples...: 23633 examples [00:07, 4462.36 examples/s]\u001b[A\n","Generating test examples...: 24083 examples [00:07, 4470.00 examples/s]\u001b[A\n","Generating test examples...: 24572 examples [00:07, 4589.97 examples/s]\u001b[A\n","                                                                       \u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-test.tfrecord*...:   0% 0/25000 [00:00<?, ? examples/s]\u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-test.tfrecord*...:  84% 21033/25000 [00:00<00:00, 210305.63 examples/s]\u001b[A\n","Generating splits...:  67% 2/3 [00:26<00:13, 13.11s/ splits]\n","Generating unsupervised examples...: 0 examples [00:00, ? examples/s]\u001b[A\n","Generating unsupervised examples...: 1 examples [00:06,  6.03s/ examples]\u001b[A\n","Generating unsupervised examples...: 473 examples [00:06, 109.38 examples/s]\u001b[A\n","Generating unsupervised examples...: 1013 examples [00:06, 278.68 examples/s]\u001b[A\n","Generating unsupervised examples...: 1519 examples [00:06, 487.65 examples/s]\u001b[A\n","Generating unsupervised examples...: 2015 examples [00:06, 750.39 examples/s]\u001b[A\n","Generating unsupervised examples...: 2500 examples [00:06, 1067.87 examples/s]\u001b[A\n","Generating unsupervised examples...: 2969 examples [00:06, 1428.45 examples/s]\u001b[A\n","Generating unsupervised examples...: 3485 examples [00:06, 1893.02 examples/s]\u001b[A\n","Generating unsupervised examples...: 3975 examples [00:06, 2346.96 examples/s]\u001b[A\n","Generating unsupervised examples...: 4472 examples [00:06, 2811.81 examples/s]\u001b[A\n","Generating unsupervised examples...: 5003 examples [00:07, 3315.83 examples/s]\u001b[A\n","Generating unsupervised examples...: 5505 examples [00:07, 3618.95 examples/s]\u001b[A\n","Generating unsupervised examples...: 5994 examples [00:07, 2679.68 examples/s]\u001b[A\n","Generating unsupervised examples...: 6475 examples [00:07, 3081.08 examples/s]\u001b[A\n","Generating unsupervised examples...: 6980 examples [00:07, 3498.75 examples/s]\u001b[A\n","Generating unsupervised examples...: 7468 examples [00:07, 3818.84 examples/s]\u001b[A\n","Generating unsupervised examples...: 7974 examples [00:07, 4127.96 examples/s]\u001b[A\n","Generating unsupervised examples...: 8490 examples [00:07, 4397.48 examples/s]\u001b[A\n","Generating unsupervised examples...: 8980 examples [00:08, 4532.54 examples/s]\u001b[A\n","Generating unsupervised examples...: 9467 examples [00:08, 4234.45 examples/s]\u001b[A\n","Generating unsupervised examples...: 9917 examples [00:08, 4139.16 examples/s]\u001b[A\n","Generating unsupervised examples...: 10392 examples [00:08, 4301.90 examples/s]\u001b[A\n","Generating unsupervised examples...: 10894 examples [00:08, 4499.98 examples/s]\u001b[A\n","Generating unsupervised examples...: 11356 examples [00:08, 4472.45 examples/s]\u001b[A\n","Generating unsupervised examples...: 11838 examples [00:08, 4567.49 examples/s]\u001b[A\n","Generating unsupervised examples...: 12337 examples [00:08, 4688.27 examples/s]\u001b[A\n","Generating unsupervised examples...: 12811 examples [00:08, 4669.11 examples/s]\u001b[A\n","Generating unsupervised examples...: 13282 examples [00:08, 4663.48 examples/s]\u001b[A\n","Generating unsupervised examples...: 13766 examples [00:09, 4715.27 examples/s]\u001b[A\n","Generating unsupervised examples...: 14240 examples [00:09, 4573.81 examples/s]\u001b[A\n","Generating unsupervised examples...: 14700 examples [00:09, 4421.14 examples/s]\u001b[A\n","Generating unsupervised examples...: 15213 examples [00:09, 4622.23 examples/s]\u001b[A\n","Generating unsupervised examples...: 15678 examples [00:09, 4574.11 examples/s]\u001b[A\n","Generating unsupervised examples...: 16138 examples [00:09, 4553.41 examples/s]\u001b[A\n","Generating unsupervised examples...: 16619 examples [00:09, 4627.73 examples/s]\u001b[A\n","Generating unsupervised examples...: 17083 examples [00:09, 4563.45 examples/s]\u001b[A\n","Generating unsupervised examples...: 17553 examples [00:09, 4601.01 examples/s]\u001b[A\n","Generating unsupervised examples...: 18022 examples [00:10, 4624.99 examples/s]\u001b[A\n","Generating unsupervised examples...: 18503 examples [00:10, 4677.84 examples/s]\u001b[A\n","Generating unsupervised examples...: 18972 examples [00:10, 4568.80 examples/s]\u001b[A\n","Generating unsupervised examples...: 19470 examples [00:10, 4687.46 examples/s]\u001b[A\n","Generating unsupervised examples...: 19940 examples [00:10, 4674.80 examples/s]\u001b[A\n","Generating unsupervised examples...: 20409 examples [00:10, 4675.10 examples/s]\u001b[A\n","Generating unsupervised examples...: 20897 examples [00:10, 4733.42 examples/s]\u001b[A\n","Generating unsupervised examples...: 21372 examples [00:10, 4736.84 examples/s]\u001b[A\n","Generating unsupervised examples...: 21846 examples [00:10, 4714.98 examples/s]\u001b[A\n","Generating unsupervised examples...: 22337 examples [00:10, 4772.48 examples/s]\u001b[A\n","Generating unsupervised examples...: 22815 examples [00:11, 4759.34 examples/s]\u001b[A\n","Generating unsupervised examples...: 23292 examples [00:11, 4752.28 examples/s]\u001b[A\n","Generating unsupervised examples...: 23768 examples [00:11, 4632.19 examples/s]\u001b[A\n","Generating unsupervised examples...: 24233 examples [00:11, 4635.29 examples/s]\u001b[A\n","Generating unsupervised examples...: 24698 examples [00:11, 4626.90 examples/s]\u001b[A\n","Generating unsupervised examples...: 25178 examples [00:11, 4676.86 examples/s]\u001b[A\n","Generating unsupervised examples...: 25646 examples [00:11, 4593.71 examples/s]\u001b[A\n","Generating unsupervised examples...: 26106 examples [00:11, 4567.74 examples/s]\u001b[A\n","Generating unsupervised examples...: 26569 examples [00:11, 4585.60 examples/s]\u001b[A\n","Generating unsupervised examples...: 27028 examples [00:11, 4550.76 examples/s]\u001b[A\n","Generating unsupervised examples...: 27497 examples [00:12, 4589.44 examples/s]\u001b[A\n","Generating unsupervised examples...: 27976 examples [00:12, 4647.74 examples/s]\u001b[A\n","Generating unsupervised examples...: 28441 examples [00:12, 4440.31 examples/s]\u001b[A\n","Generating unsupervised examples...: 28890 examples [00:12, 4452.01 examples/s]\u001b[A\n","Generating unsupervised examples...: 29357 examples [00:12, 4509.35 examples/s]\u001b[A\n","Generating unsupervised examples...: 29832 examples [00:12, 4577.44 examples/s]\u001b[A\n","Generating unsupervised examples...: 30291 examples [00:12, 4451.40 examples/s]\u001b[A\n","Generating unsupervised examples...: 30738 examples [00:12, 4438.45 examples/s]\u001b[A\n","Generating unsupervised examples...: 31183 examples [00:12, 4414.91 examples/s]\u001b[A\n","Generating unsupervised examples...: 31658 examples [00:12, 4513.06 examples/s]\u001b[A\n","Generating unsupervised examples...: 32122 examples [00:13, 4549.48 examples/s]\u001b[A\n","Generating unsupervised examples...: 32585 examples [00:13, 4570.49 examples/s]\u001b[A\n","Generating unsupervised examples...: 33043 examples [00:13, 4470.70 examples/s]\u001b[A\n","Generating unsupervised examples...: 33491 examples [00:13, 4411.84 examples/s]\u001b[A\n","Generating unsupervised examples...: 33957 examples [00:13, 4481.54 examples/s]\u001b[A\n","Generating unsupervised examples...: 34419 examples [00:13, 4521.33 examples/s]\u001b[A\n","Generating unsupervised examples...: 34872 examples [00:13, 4495.68 examples/s]\u001b[A\n","Generating unsupervised examples...: 35322 examples [00:13, 4458.60 examples/s]\u001b[A\n","Generating unsupervised examples...: 35780 examples [00:13, 4491.36 examples/s]\u001b[A\n","Generating unsupervised examples...: 36230 examples [00:14, 4482.14 examples/s]\u001b[A\n","Generating unsupervised examples...: 36692 examples [00:14, 4521.76 examples/s]\u001b[A\n","Generating unsupervised examples...: 37151 examples [00:14, 4539.74 examples/s]\u001b[A\n","Generating unsupervised examples...: 37606 examples [00:14, 4372.68 examples/s]\u001b[A\n","Generating unsupervised examples...: 38053 examples [00:14, 4400.07 examples/s]\u001b[A\n","Generating unsupervised examples...: 38511 examples [00:14, 4451.33 examples/s]\u001b[A\n","Generating unsupervised examples...: 38957 examples [00:14, 4416.49 examples/s]\u001b[A\n","Generating unsupervised examples...: 39400 examples [00:14, 4292.37 examples/s]\u001b[A\n","Generating unsupervised examples...: 39843 examples [00:14, 4329.71 examples/s]\u001b[A\n","Generating unsupervised examples...: 40286 examples [00:14, 4358.82 examples/s]\u001b[A\n","Generating unsupervised examples...: 40735 examples [00:15, 4397.01 examples/s]\u001b[A\n","Generating unsupervised examples...: 41197 examples [00:15, 4459.25 examples/s]\u001b[A\n","Generating unsupervised examples...: 41652 examples [00:15, 4481.89 examples/s]\u001b[A\n","Generating unsupervised examples...: 42101 examples [00:15, 4399.15 examples/s]\u001b[A\n","Generating unsupervised examples...: 42593 examples [00:15, 4550.81 examples/s]\u001b[A\n","Generating unsupervised examples...: 43049 examples [00:15, 4549.98 examples/s]\u001b[A\n","Generating unsupervised examples...: 43509 examples [00:15, 4563.70 examples/s]\u001b[A\n","Generating unsupervised examples...: 43991 examples [00:15, 4639.78 examples/s]\u001b[A\n","Generating unsupervised examples...: 44456 examples [00:15, 4307.96 examples/s]\u001b[A\n","Generating unsupervised examples...: 44892 examples [00:16, 3754.66 examples/s]\u001b[A\n","Generating unsupervised examples...: 45283 examples [00:16, 3422.07 examples/s]\u001b[A\n","Generating unsupervised examples...: 45639 examples [00:16, 3309.72 examples/s]\u001b[A\n","Generating unsupervised examples...: 45979 examples [00:16, 3211.28 examples/s]\u001b[A\n","Generating unsupervised examples...: 46306 examples [00:16, 3147.64 examples/s]\u001b[A\n","Generating unsupervised examples...: 46625 examples [00:16, 3000.55 examples/s]\u001b[A\n","Generating unsupervised examples...: 46928 examples [00:16, 3004.76 examples/s]\u001b[A\n","Generating unsupervised examples...: 47248 examples [00:16, 3057.67 examples/s]\u001b[A\n","Generating unsupervised examples...: 47556 examples [00:16, 3018.10 examples/s]\u001b[A\n","Generating unsupervised examples...: 47871 examples [00:17, 3051.91 examples/s]\u001b[A\n","Generating unsupervised examples...: 48178 examples [00:17, 3036.93 examples/s]\u001b[A\n","Generating unsupervised examples...: 48493 examples [00:17, 3069.54 examples/s]\u001b[A\n","Generating unsupervised examples...: 48801 examples [00:17, 3053.16 examples/s]\u001b[A\n","Generating unsupervised examples...: 49118 examples [00:17, 3085.54 examples/s]\u001b[A\n","Generating unsupervised examples...: 49431 examples [00:17, 3096.75 examples/s]\u001b[A\n","Generating unsupervised examples...: 49752 examples [00:17, 3129.65 examples/s]\u001b[A\n","                                                                               \u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-unsupervised.tfrecord*...:   0% 0/50000 [00:00<?, ? examples/s]\u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-unsupervised.tfrecord*...:  11% 5255/50000 [00:00<00:00, 52542.78 examples/s]\u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-unsupervised.tfrecord*...:  42% 21226/50000 [00:00<00:00, 115568.13 examples/s]\u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-unsupervised.tfrecord*...:  69% 34544/50000 [00:00<00:00, 123596.30 examples/s]\u001b[A\n","Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5ZDOX3/imdb_reviews-unsupervised.tfrecord*...:  94% 46904/50000 [00:00<00:00, 120238.16 examples/s]\u001b[A\n","\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","2023-04-23 05:10:22.715515: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n","\t [[{{node Placeholder/_3}}]]\n","2023-04-23 05:10:22.716592: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n","\t [[{{node Placeholder/_4}}]]\n","2023-04-23 05:10:22.808317: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","2023-04-23 05:10:23.108075: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n","\t [[{{node Placeholder/_2}}]]\n","2023-04-23 05:10:23.109196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n","\t [[{{node Placeholder/_0}}]]\n","2023-04-23 05:10:53.280871: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n","\t [[{{node Placeholder/_0}}]]\n","2023-04-23 05:10:53.281745: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n","\t [[{{node Placeholder/_2}}]]\n","2023-04-23 05:11:26.232021: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n","\t [[{{node Placeholder/_3}}]]\n","2023-04-23 05:11:26.232735: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n","\t [[{{node Placeholder/_4}}]]\n"]}],"source":["!python3 \"text.py\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wW4nVG9qNShW","executionInfo":{"status":"ok","timestamp":1682505832588,"user_tz":420,"elapsed":3646467,"user":{"displayName":"Li An","userId":"00679100983466103760"}},"outputId":"e14c287b-0229-4127-957a-9d73c0dc1bf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["position: 2000\n","layer:1 \t distance:3 \t size:3\n","layer:2 \t distance:7 \t size:7\n","layer:3 \t distance:15 \t size:15\n","layer:4 \t distance:31 \t size:31\n","layer:5 \t distance:63 \t size:63\n","layer:6 \t distance:127 \t size:127\n","layer:7 \t distance:255 \t size:255\n","layer:8 \t distance:511 \t size:511\n","layer:9 \t distance:1023 \t size:1023\n","layer:10 \t distance:2047 \t size:2047\n","layer:11 \t distance:4000 \t size:4000\n","\n","CONV(\n","  (embedding): Embedding(256, 64)\n","  (conv): ConvPart(\n","    (conv_net): Sequential(\n","      (0): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (1): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (2): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (3): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (4): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (5): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (6): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (7): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (8): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(256,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (9): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(512,), dilation=(512,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (10): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1024,), dilation=(1024,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","    )\n","  )\n","  (attention): SelfAttention(\n","    (query): Conv1d(64, 8, kernel_size=(1,), stride=(1,), bias=False)\n","    (key): Conv1d(64, 8, kernel_size=(1,), stride=(1,), bias=False)\n","    (value): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n","  )\n","  (linear): Linear(in_features=64, out_features=2, bias=True)\n",")\n","Tesla T4\n","text_4000_P158291_CDIL_S1_L11_H64\n","Loaded ./lra_datasets/text_4000.train.pickle... size=25000\n","Loaded ./lra_datasets/text_4000.dev.pickle... size=25000\n","Loaded ./lra_datasets/text_4000.test.pickle... size=25000\n","100% 782/782 [04:33<00:00,  2.86it/s]\n","Epoch: 0\n","Train num: 25000 — Train loss: 0.7047947500038148 — Time: 273.332041\n","100% 782/782 [01:19<00:00,  9.87it/s]\n","Val num: 25000 — Val loss: 0.6773932513237 — Val accuracy: 53.588 — Time: 79.256658\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.86it/s]\n","Test num: 25000 — Test loss: 0.6773932528495789 — Test accuracy: 53.588 — Time: 79.31207\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.90it/s]\n","Epoch: 1\n","Train num: 25000 — Train loss: 0.6365360677146912 — Time: 270.095647\n","100% 782/782 [01:21<00:00,  9.63it/s]\n","Val num: 25000 — Val loss: 0.5925888304901124 — Val accuracy: 69.128 — Time: 81.23003\n","________________________________________________________________________________\n","100% 782/782 [01:21<00:00,  9.64it/s]\n","Test num: 25000 — Test loss: 0.5925888277626038 — Test accuracy: 69.128 — Time: 81.125211\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 2\n","Train num: 25000 — Train loss: 0.5617928371810913 — Time: 270.479809\n","100% 782/782 [01:22<00:00,  9.51it/s]\n","Val num: 25000 — Val loss: 0.5172480241775512 — Val accuracy: 75.16000000000001 — Time: 82.243396\n","________________________________________________________________________________\n","100% 782/782 [01:22<00:00,  9.53it/s]\n","Test num: 25000 — Test loss: 0.5172480225944519 — Test accuracy: 75.16000000000001 — Time: 82.05173\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:29<00:00,  2.90it/s]\n","Epoch: 3\n","Train num: 25000 — Train loss: 0.49488319427490235 — Time: 269.826698\n","100% 782/782 [01:22<00:00,  9.44it/s]\n","Val num: 25000 — Val loss: 0.4448416396522522 — Val accuracy: 79.296 — Time: 82.833571\n","________________________________________________________________________________\n","100% 782/782 [01:22<00:00,  9.44it/s]\n","Test num: 25000 — Test loss: 0.4448416390800476 — Test accuracy: 79.296 — Time: 82.83298\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 4\n","Train num: 25000 — Train loss: 0.4246831283664703 — Time: 270.473276\n","100% 782/782 [01:22<00:00,  9.46it/s]\n","Val num: 25000 — Val loss: 0.380053067855835 — Val accuracy: 82.968 — Time: 82.707567\n","________________________________________________________________________________\n","100% 782/782 [01:22<00:00,  9.46it/s]\n","Test num: 25000 — Test loss: 0.38005306512832643 — Test accuracy: 82.968 — Time: 82.625164\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 5\n","Train num: 25000 — Train loss: 0.3594384806346893 — Time: 270.703556\n","100% 782/782 [01:22<00:00,  9.44it/s]\n","Val num: 25000 — Val loss: 0.38971169335365297 — Val accuracy: 81.372 — Time: 82.825182\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 6\n","Train num: 25000 — Train loss: 0.3319036821460724 — Time: 270.555747\n","100% 782/782 [01:22<00:00,  9.42it/s]\n","Val num: 25000 — Val loss: 0.3551336842727661 — Val accuracy: 84.20400000000001 — Time: 82.995833\n","________________________________________________________________________________\n","100% 782/782 [01:23<00:00,  9.40it/s]\n","Test num: 25000 — Test loss: 0.355133683681488 — Test accuracy: 84.20400000000001 — Time: 83.208573\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 7\n","Train num: 25000 — Train loss: 0.3056379583740234 — Time: 270.585667\n","100% 782/782 [01:23<00:00,  9.34it/s]\n","Val num: 25000 — Val loss: 0.3166742098903656 — Val accuracy: 86.392 — Time: 83.756114\n","________________________________________________________________________________\n","100% 782/782 [01:23<00:00,  9.37it/s]\n","Test num: 25000 — Test loss: 0.31667421189308165 — Test accuracy: 86.392 — Time: 83.435949\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.90it/s]\n","Epoch: 8\n","Train num: 25000 — Train loss: 0.29457540760993955 — Time: 270.008284\n","100% 782/782 [01:24<00:00,  9.30it/s]\n","Val num: 25000 — Val loss: 0.41264285016059876 — Val accuracy: 81.08800000000001 — Time: 84.126856\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.90it/s]\n","Epoch: 9\n","Train num: 25000 — Train loss: 0.277448824338913 — Time: 270.087888\n","100% 782/782 [01:23<00:00,  9.33it/s]\n","Val num: 25000 — Val loss: 0.29650878976345063 — Val accuracy: 87.41600000000001 — Time: 83.810017\n","________________________________________________________________________________\n","100% 782/782 [01:23<00:00,  9.36it/s]\n","Test num: 25000 — Test loss: 0.29650878984451295 — Test accuracy: 87.41600000000001 — Time: 83.537841\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.90it/s]\n","Epoch: 10\n","Train num: 25000 — Train loss: 0.2612683399772644 — Time: 270.05255\n","100% 782/782 [01:24<00:00,  9.27it/s]\n","Val num: 25000 — Val loss: 0.3098362763595581 — Val accuracy: 87.412 — Time: 84.338324\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 11\n","Train num: 25000 — Train loss: 0.25044140703201295 — Time: 270.270982\n","100% 782/782 [01:23<00:00,  9.34it/s]\n","Val num: 25000 — Val loss: 0.3935798622751236 — Val accuracy: 83.176 — Time: 83.759384\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 12\n","Train num: 25000 — Train loss: 0.2449109302663803 — Time: 270.56018\n","100% 782/782 [01:23<00:00,  9.35it/s]\n","Val num: 25000 — Val loss: 0.49961445161819457 — Val accuracy: 78.62 — Time: 83.672578\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 13\n","Train num: 25000 — Train loss: 0.22772065796375274 — Time: 270.548758\n","100% 782/782 [01:23<00:00,  9.41it/s]\n","Val num: 25000 — Val loss: 0.36962432224273684 — Val accuracy: 83.104 — Time: 83.103113\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 14\n","Train num: 25000 — Train loss: 0.22495255195379257 — Time: 270.831607\n","100% 782/782 [01:24<00:00,  9.30it/s]\n","Val num: 25000 — Val loss: 0.3496403377056122 — Val accuracy: 86.568 — Time: 84.068269\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 15\n","Train num: 25000 — Train loss: 0.1997037428998947 — Time: 270.583911\n","100% 782/782 [01:23<00:00,  9.38it/s]\n","Val num: 25000 — Val loss: 0.3722992584133148 — Val accuracy: 84.884 — Time: 83.361935\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 16\n","Train num: 25000 — Train loss: 0.19635209612846374 — Time: 270.539886\n","100% 782/782 [01:23<00:00,  9.31it/s]\n","Val num: 25000 — Val loss: 0.3420736499881744 — Val accuracy: 87.076 — Time: 83.96598\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 17\n","Train num: 25000 — Train loss: 0.1798513759279251 — Time: 270.554424\n","100% 782/782 [01:23<00:00,  9.36it/s]\n","Val num: 25000 — Val loss: 0.4043925675868988 — Val accuracy: 85.532 — Time: 83.521637\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 18\n","Train num: 25000 — Train loss: 0.15708592207312583 — Time: 270.71915\n","100% 782/782 [01:24<00:00,  9.29it/s]\n","Val num: 25000 — Val loss: 0.4249763839840889 — Val accuracy: 86.172 — Time: 84.188945\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 19\n","Train num: 25000 — Train loss: 0.1505485027754307 — Time: 270.604457\n","100% 782/782 [01:23<00:00,  9.38it/s]\n","Val num: 25000 — Val loss: 0.3904910773229599 — Val accuracy: 86.636 — Time: 83.343299\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 20\n","Train num: 25000 — Train loss: 0.1431545575273037 — Time: 270.928502\n","100% 782/782 [01:23<00:00,  9.33it/s]\n","Val num: 25000 — Val loss: 0.39337985576815904 — Val accuracy: 86.876 — Time: 83.812098\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 21\n","Train num: 25000 — Train loss: 0.1276083589875698 — Time: 270.750674\n","100% 782/782 [01:24<00:00,  9.29it/s]\n","Val num: 25000 — Val loss: 0.4159554487645626 — Val accuracy: 87.412 — Time: 84.19552\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.89it/s]\n","Epoch: 22\n","Train num: 25000 — Train loss: 0.11353116937756538 — Time: 271.020443\n","100% 782/782 [01:22<00:00,  9.44it/s]\n","Val num: 25000 — Val loss: 0.4183731381583214 — Val accuracy: 85.528 — Time: 82.821113\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 23\n","Train num: 25000 — Train loss: 0.10266060405388475 — Time: 270.865368\n","100% 782/782 [01:23<00:00,  9.36it/s]\n","Val num: 25000 — Val loss: 0.5244608070373535 — Val accuracy: 86.016 — Time: 83.590115\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 24\n","Train num: 25000 — Train loss: 0.09443403580635785 — Time: 270.963128\n","100% 782/782 [01:23<00:00,  9.35it/s]\n","Val num: 25000 — Val loss: 0.5394575249958038 — Val accuracy: 84.872 — Time: 83.655256\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 25\n","Train num: 25000 — Train loss: 0.09220035676896572 — Time: 270.833394\n","100% 782/782 [01:22<00:00,  9.42it/s]\n","Val num: 25000 — Val loss: 0.4765258419680595 — Val accuracy: 86.412 — Time: 82.987065\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 26\n","Train num: 25000 — Train loss: 0.07615494154810905 — Time: 270.727362\n","100% 782/782 [01:23<00:00,  9.32it/s]\n","Val num: 25000 — Val loss: 0.6405611781024932 — Val accuracy: 84.744 — Time: 83.904431\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.89it/s]\n","Epoch: 27\n","Train num: 25000 — Train loss: 0.06295516462493687 — Time: 271.054921\n","100% 782/782 [01:24<00:00,  9.29it/s]\n","Val num: 25000 — Val loss: 0.8572700419330597 — Val accuracy: 82.57600000000001 — Time: 84.136379\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.89it/s]\n","Epoch: 28\n","Train num: 25000 — Train loss: 0.06975389742940664 — Time: 271.027465\n","100% 782/782 [01:24<00:00,  9.29it/s]\n","Val num: 25000 — Val loss: 0.5511282223248481 — Val accuracy: 86.48400000000001 — Time: 84.202552\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 29\n","Train num: 25000 — Train loss: 0.06429664701029658 — Time: 271.734142\n","100% 782/782 [01:24<00:00,  9.24it/s]\n","Val num: 25000 — Val loss: 0.6879104658472538 — Val accuracy: 84.852 — Time: 84.606895\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 30\n","Train num: 25000 — Train loss: 0.06189957106510177 — Time: 271.420411\n","100% 782/782 [01:23<00:00,  9.34it/s]\n","Val num: 25000 — Val loss: 0.5205779814136028 — Val accuracy: 86.056 — Time: 83.711176\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 31\n","Train num: 25000 — Train loss: 0.06767497823141981 — Time: 270.747624\n","100% 782/782 [01:23<00:00,  9.39it/s]\n","Val num: 25000 — Val loss: 0.4995616469979286 — Val accuracy: 86.64 — Time: 83.317272\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 32\n","Train num: 25000 — Train loss: 0.046620266883615404 — Time: 271.183531\n","100% 782/782 [01:24<00:00,  9.24it/s]\n","Val num: 25000 — Val loss: 0.7148609890598059 — Val accuracy: 86.212 — Time: 84.650387\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 33\n","Train num: 25000 — Train loss: 0.05390972712178715 — Time: 270.904626\n","100% 782/782 [01:22<00:00,  9.46it/s]\n","Val num: 25000 — Val loss: 0.4667520444440842 — Val accuracy: 85.604 — Time: 82.693411\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 34\n","Train num: 25000 — Train loss: 0.04678350989883766 — Time: 271.206024\n","100% 782/782 [01:24<00:00,  9.28it/s]\n","Val num: 25000 — Val loss: 0.6649965361189842 — Val accuracy: 86.044 — Time: 84.29182\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 35\n","Train num: 25000 — Train loss: 0.03776376204818487 — Time: 271.493608\n","100% 782/782 [01:24<00:00,  9.22it/s]\n","Val num: 25000 — Val loss: 0.7965346131134033 — Val accuracy: 86.33999999999999 — Time: 84.838038\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 36\n","Train num: 25000 — Train loss: 0.04490150565473363 — Time: 271.555955\n","100% 782/782 [01:24<00:00,  9.25it/s]\n","Val num: 25000 — Val loss: 0.7053916276769061 — Val accuracy: 85.232 — Time: 84.50251\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 37\n","Train num: 25000 — Train loss: 0.047194841511253034 — Time: 271.69286\n","100% 782/782 [01:23<00:00,  9.38it/s]\n","Val num: 25000 — Val loss: 0.568640536878705 — Val accuracy: 86.384 — Time: 83.378928\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 38\n","Train num: 25000 — Train loss: 0.03631259306360036 — Time: 271.694255\n","100% 782/782 [01:24<00:00,  9.24it/s]\n","Val num: 25000 — Val loss: 0.7332165925562382 — Val accuracy: 86.464 — Time: 84.615918\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 39\n","Train num: 25000 — Train loss: 0.03870075675912201 — Time: 271.74332\n","100% 782/782 [01:24<00:00,  9.24it/s]\n","Val num: 25000 — Val loss: 0.941318176304847 — Val accuracy: 84.552 — Time: 84.668193\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 40\n","Train num: 25000 — Train loss: 0.04229900939244428 — Time: 271.803244\n","100% 782/782 [01:24<00:00,  9.23it/s]\n","Val num: 25000 — Val loss: 0.7120442570024729 — Val accuracy: 86.536 — Time: 84.711219\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 41\n","Train num: 25000 — Train loss: 0.037957891499637623 — Time: 271.407526\n","100% 782/782 [01:24<00:00,  9.24it/s]\n","Val num: 25000 — Val loss: 0.7186258307319879 — Val accuracy: 86.524 — Time: 84.669463\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.89it/s]\n","Epoch: 42\n","Train num: 25000 — Train loss: 0.029498309022658504 — Time: 271.053783\n","100% 782/782 [01:24<00:00,  9.23it/s]\n","Val num: 25000 — Val loss: 0.7707549025988579 — Val accuracy: 85.868 — Time: 84.723037\n","________________________________________________________________________________\n","100% 782/782 [04:32<00:00,  2.87it/s]\n","Epoch: 43\n","Train num: 25000 — Train loss: 0.04000482112079859 — Time: 272.086374\n","100% 782/782 [01:24<00:00,  9.25it/s]\n","Val num: 25000 — Val loss: 0.7921579740363359 — Val accuracy: 86.492 — Time: 84.583858\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 44\n","Train num: 25000 — Train loss: 0.028217235448220745 — Time: 270.906608\n","100% 782/782 [01:24<00:00,  9.22it/s]\n","Val num: 25000 — Val loss: 0.8863987486842275 — Val accuracy: 86.436 — Time: 84.817233\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 45\n","Train num: 25000 — Train loss: 0.03849078156365547 — Time: 271.595809\n","100% 782/782 [01:23<00:00,  9.33it/s]\n","Val num: 25000 — Val loss: 0.6898637126198411 — Val accuracy: 86.016 — Time: 83.796022\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 46\n","Train num: 25000 — Train loss: 0.030859567164098843 — Time: 271.314384\n","100% 782/782 [01:24<00:00,  9.21it/s]\n","Val num: 25000 — Val loss: 0.859656109777689 — Val accuracy: 86.404 — Time: 84.88519\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 47\n","Train num: 25000 — Train loss: 0.02641075280683115 — Time: 271.432058\n","100% 782/782 [01:24<00:00,  9.21it/s]\n","Val num: 25000 — Val loss: 0.9895726522183418 — Val accuracy: 85.572 — Time: 84.902284\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 48\n","Train num: 25000 — Train loss: 0.02575901001510676 — Time: 271.58192\n","100% 782/782 [01:24<00:00,  9.23it/s]\n","Val num: 25000 — Val loss: 0.9963613352870941 — Val accuracy: 85.384 — Time: 84.741492\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 49\n","Train num: 25000 — Train loss: 0.0357770103697665 — Time: 271.624943\n","100% 782/782 [01:24<00:00,  9.22it/s]\n","Val num: 25000 — Val loss: 0.8997966563618183 — Val accuracy: 86.164 — Time: 84.817574\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 50\n","Train num: 25000 — Train loss: 0.017118940776793753 — Time: 271.305756\n","100% 782/782 [01:24<00:00,  9.25it/s]\n","Val num: 25000 — Val loss: 0.9072604066967964 — Val accuracy: 85.188 — Time: 84.544522\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 51\n","Train num: 25000 — Train loss: 0.027534920149473473 — Time: 271.42546\n","100% 782/782 [01:24<00:00,  9.21it/s]\n","Val num: 25000 — Val loss: 0.9855564654028416 — Val accuracy: 85.896 — Time: 84.865108\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 52\n","Train num: 25000 — Train loss: 0.030043576504956 — Time: 271.299588\n","100% 782/782 [01:24<00:00,  9.24it/s]\n","Val num: 25000 — Val loss: 0.806690751633048 — Val accuracy: 86.32 — Time: 84.621685\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 53\n","Train num: 25000 — Train loss: 0.02640976126064197 — Time: 271.38773\n","100% 782/782 [01:23<00:00,  9.37it/s]\n","Val num: 25000 — Val loss: 0.7126943712592125 — Val accuracy: 85.71600000000001 — Time: 83.47776\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 54\n","Train num: 25000 — Train loss: 0.02407197666642256 — Time: 271.310307\n","100% 782/782 [01:24<00:00,  9.26it/s]\n","Val num: 25000 — Val loss: 0.8823941834276915 — Val accuracy: 85.63199999999999 — Time: 84.446735\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 55\n","Train num: 25000 — Train loss: 0.022802426414615475 — Time: 271.281847\n","100% 782/782 [01:23<00:00,  9.34it/s]\n","Val num: 25000 — Val loss: 0.6890545520508289 — Val accuracy: 86.076 — Time: 83.740459\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 56\n","Train num: 25000 — Train loss: 0.021341479746472325 — Time: 271.43422\n","100% 782/782 [01:24<00:00,  9.22it/s]\n","Val num: 25000 — Val loss: 1.004295326972145 — Val accuracy: 85.776 — Time: 84.81806\n","________________________________________________________________________________\n","100% 782/782 [04:30<00:00,  2.89it/s]\n","Epoch: 57\n","Train num: 25000 — Train loss: 0.019968564545802946 — Time: 270.979548\n","100% 782/782 [01:24<00:00,  9.24it/s]\n","Val num: 25000 — Val loss: 0.973224706871286 — Val accuracy: 86.59599999999999 — Time: 84.665235\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 58\n","Train num: 25000 — Train loss: 0.021166776374685142 — Time: 271.225293\n","100% 782/782 [01:24<00:00,  9.23it/s]\n","Val num: 25000 — Val loss: 0.9559641791498102 — Val accuracy: 85.956 — Time: 84.688481\n","________________________________________________________________________________\n","100% 782/782 [04:31<00:00,  2.88it/s]\n","Epoch: 59\n","Train num: 25000 — Train loss: 0.019519140334880794 — Time: 271.651394\n","100% 782/782 [01:24<00:00,  9.28it/s]\n","Val num: 25000 — Val loss: 0.81595683177948 — Val accuracy: 84.648 — Time: 84.300942\n","________________________________________________________________________________\n","100% 782/782 [04:32<00:00,  2.87it/s]\n","Epoch: 60\n","Train num: 25000 — Train loss: 0.023162702939733864 — Time: 272.00143\n","100% 782/782 [01:24<00:00,  9.22it/s]\n","Val num: 25000 — Val loss: 0.9188703895565867 — Val accuracy: 86.82 — Time: 84.795449\n","________________________________________________________________________________\n","  3% 25/782 [00:09<04:33,  2.77it/s]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/CSCI544/project/CDIL-CNN-main/experiments/_2lra/lra_main.py\", line 113, in <module>\n","    TrainModel(\n","  File \"/content/drive/MyDrive/CSCI544/project/CDIL-CNN-main/experiments/_2lra/lra_train.py\", line 106, in TrainModel\n","    batch_loss.backward()\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","^C\n"]}],"source":["# attention 100\n","!python3 \"lra_main.py\" --task text_4000 --model CDIL"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10999689,"status":"ok","timestamp":1682321756272,"user":{"displayName":"Li An","userId":"00679100983466103760"},"user_tz":420},"id":"NG_Rx7oqW7iy","outputId":"a15ac999-84e7-45b0-9bd8-1a735f734b88"},"outputs":[{"name":"stdout","output_type":"stream","text":["position: 2000\n","layer:1 \t distance:3 \t size:3\n","layer:2 \t distance:7 \t size:7\n","layer:3 \t distance:15 \t size:15\n","layer:4 \t distance:31 \t size:31\n","layer:5 \t distance:63 \t size:63\n","layer:6 \t distance:127 \t size:127\n","layer:7 \t distance:255 \t size:255\n","layer:8 \t distance:511 \t size:511\n","layer:9 \t distance:1023 \t size:1023\n","layer:10 \t distance:2047 \t size:2047\n","layer:11 \t distance:4000 \t size:4000\n","\n","CONV(\n","  (embedding): Embedding(256, 64)\n","  (conv): ConvPart(\n","    (conv_net): Sequential(\n","      (0): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (1): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (2): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (3): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (4): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (5): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (6): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (7): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (8): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(256,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (9): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(512,), dilation=(512,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (10): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1024,), dilation=(1024,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","    )\n","  )\n","  (attention): SelfAttention(\n","    (query): Conv1d(64, 8, kernel_size=(1,), stride=(1,), bias=False)\n","    (key): Conv1d(64, 8, kernel_size=(1,), stride=(1,), bias=False)\n","    (value): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n","  )\n","  (linear): Linear(in_features=64, out_features=2, bias=True)\n",")\n","Tesla T4\n","text_4000_P158291_CDIL_S1_L11_H64\n","Loaded ./lra_datasets/text_4000.train.pickle... size=25000\n","Loaded ./lra_datasets/text_4000.dev.pickle... size=25000\n","Loaded ./lra_datasets/text_4000.test.pickle... size=25000\n","100% 782/782 [04:19<00:00,  3.01it/s]\n","Epoch: 0\n","Train num: 25000 — Train loss: 0.7061315285873413 — Time: 259.524598\n","100% 782/782 [01:18<00:00, 10.00it/s]\n","Val num: 25000 — Val loss: 0.6932888518714905 — Val accuracy: 50.404 — Time: 78.232277\n","________________________________________________________________________________\n","100% 782/782 [01:18<00:00,  9.99it/s]\n","Test num: 25000 — Test loss: 0.6932888480567932 — Test accuracy: 50.404 — Time: 78.296695\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:14<00:00,  3.07it/s]\n","Epoch: 1\n","Train num: 25000 — Train loss: 0.6452793963623047 — Time: 254.462724\n","100% 782/782 [01:19<00:00,  9.88it/s]\n","Val num: 25000 — Val loss: 0.60790961643219 — Val accuracy: 67.58 — Time: 79.152237\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.89it/s]\n","Test num: 25000 — Test loss: 0.6079096122360229 — Test accuracy: 67.58 — Time: 79.108895\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:15<00:00,  3.06it/s]\n","Epoch: 2\n","Train num: 25000 — Train loss: 0.5683604332447052 — Time: 255.325722\n","100% 782/782 [01:19<00:00,  9.89it/s]\n","Val num: 25000 — Val loss: 0.5216885647869111 — Val accuracy: 74.83999999999999 — Time: 79.093133\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.88it/s]\n","Test num: 25000 — Test loss: 0.5216885657501221 — Test accuracy: 74.83999999999999 — Time: 79.11259\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:15<00:00,  3.06it/s]\n","Epoch: 3\n","Train num: 25000 — Train loss: 0.5031664307689667 — Time: 255.289696\n","100% 782/782 [01:19<00:00,  9.87it/s]\n","Val num: 25000 — Val loss: 0.4574006964969635 — Val accuracy: 78.508 — Time: 79.23687\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.88it/s]\n","Test num: 25000 — Test loss: 0.45740069721221926 — Test accuracy: 78.508 — Time: 79.173744\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:15<00:00,  3.06it/s]\n","Epoch: 4\n","Train num: 25000 — Train loss: 0.43195868951797484 — Time: 255.519981\n","100% 782/782 [01:19<00:00,  9.87it/s]\n","Val num: 25000 — Val loss: 0.3852837620162964 — Val accuracy: 83.328 — Time: 79.222058\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.87it/s]\n","Test num: 25000 — Test loss: 0.3852837637615204 — Test accuracy: 83.328 — Time: 79.198968\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:15<00:00,  3.06it/s]\n","Epoch: 5\n","Train num: 25000 — Train loss: 0.3671122632789612 — Time: 255.805622\n","100% 782/782 [01:19<00:00,  9.88it/s]\n","Val num: 25000 — Val loss: 0.39694327108383176 — Val accuracy: 81.384 — Time: 79.135244\n","________________________________________________________________________________\n","100% 782/782 [04:15<00:00,  3.05it/s]\n","Epoch: 6\n","Train num: 25000 — Train loss: 0.3383705776691437 — Time: 255.976324\n","100% 782/782 [01:19<00:00,  9.84it/s]\n","Val num: 25000 — Val loss: 0.3557982423210144 — Val accuracy: 84.236 — Time: 79.496392\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.81it/s]\n","Test num: 25000 — Test loss: 0.3557982436180115 — Test accuracy: 84.236 — Time: 79.711223\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 7\n","Train num: 25000 — Train loss: 0.31171319721221924 — Time: 256.035766\n","100% 782/782 [01:19<00:00,  9.78it/s]\n","Val num: 25000 — Val loss: 0.3279858029937744 — Val accuracy: 85.832 — Time: 79.997039\n","________________________________________________________________________________\n","100% 782/782 [01:20<00:00,  9.77it/s]\n","Test num: 25000 — Test loss: 0.32798580414772033 — Test accuracy: 85.832 — Time: 80.021737\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 8\n","Train num: 25000 — Train loss: 0.3009056494617462 — Time: 256.094912\n","100% 782/782 [01:19<00:00,  9.81it/s]\n","Val num: 25000 — Val loss: 0.4200505262374878 — Val accuracy: 80.608 — Time: 79.712644\n","________________________________________________________________________________\n","100% 782/782 [04:15<00:00,  3.06it/s]\n","Epoch: 9\n","Train num: 25000 — Train loss: 0.2830048169088364 — Time: 255.854863\n","100% 782/782 [01:19<00:00,  9.84it/s]\n","Val num: 25000 — Val loss: 0.3189761791467667 — Val accuracy: 85.98400000000001 — Time: 79.498756\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.85it/s]\n","Test num: 25000 — Test loss: 0.3189761785697937 — Test accuracy: 85.98400000000001 — Time: 79.412125\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 10\n","Train num: 25000 — Train loss: 0.2662718980407715 — Time: 256.134271\n","100% 782/782 [01:20<00:00,  9.70it/s]\n","Val num: 25000 — Val loss: 0.29899364263534545 — Val accuracy: 87.272 — Time: 80.633421\n","________________________________________________________________________________\n","100% 782/782 [01:20<00:00,  9.73it/s]\n","Test num: 25000 — Test loss: 0.29899364298820497 — Test accuracy: 87.272 — Time: 80.397682\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 11\n","Train num: 25000 — Train loss: 0.25902443255662916 — Time: 256.449086\n","100% 782/782 [01:20<00:00,  9.76it/s]\n","Val num: 25000 — Val loss: 0.47410902938842775 — Val accuracy: 80.508 — Time: 80.155043\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 12\n","Train num: 25000 — Train loss: 0.25024232023715975 — Time: 256.523161\n","100% 782/782 [01:19<00:00,  9.80it/s]\n","Val num: 25000 — Val loss: 0.43936726249694824 — Val accuracy: 80.72399999999999 — Time: 79.755654\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 13\n","Train num: 25000 — Train loss: 0.23621892181873322 — Time: 256.469388\n","100% 782/782 [01:20<00:00,  9.77it/s]\n","Val num: 25000 — Val loss: 0.2988184606552124 — Val accuracy: 87.224 — Time: 80.059856\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 14\n","Train num: 25000 — Train loss: 0.23344206698834896 — Time: 256.430468\n","100% 782/782 [01:20<00:00,  9.75it/s]\n","Val num: 25000 — Val loss: 0.3486011184930801 — Val accuracy: 86.74 — Time: 80.206559\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 15\n","Train num: 25000 — Train loss: 0.2105424549627304 — Time: 256.442906\n","100% 782/782 [01:20<00:00,  9.76it/s]\n","Val num: 25000 — Val loss: 0.3591828370809555 — Val accuracy: 86.176 — Time: 80.125192\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 16\n","Train num: 25000 — Train loss: 0.21202582802295686 — Time: 256.633178\n","100% 782/782 [01:19<00:00,  9.86it/s]\n","Val num: 25000 — Val loss: 0.3666029948329926 — Val accuracy: 83.67599999999999 — Time: 79.328862\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 17\n","Train num: 25000 — Train loss: 0.19252607920825482 — Time: 256.571678\n","100% 782/782 [01:20<00:00,  9.73it/s]\n","Val num: 25000 — Val loss: 0.3670765331619978 — Val accuracy: 86.35199999999999 — Time: 80.351615\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 18\n","Train num: 25000 — Train loss: 0.18563192376375198 — Time: 256.693659\n","100% 782/782 [01:20<00:00,  9.77it/s]\n","Val num: 25000 — Val loss: 0.341740572412014 — Val accuracy: 87.412 — Time: 80.068263\n","________________________________________________________________________________\n","100% 782/782 [01:20<00:00,  9.76it/s]\n","Test num: 25000 — Test loss: 0.3417405710029602 — Test accuracy: 87.412 — Time: 80.121271\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 19\n","Train num: 25000 — Train loss: 0.17176991719722748 — Time: 256.670335\n","100% 782/782 [01:19<00:00,  9.78it/s]\n","Val num: 25000 — Val loss: 0.33809057175159457 — Val accuracy: 87.58 — Time: 79.928484\n","________________________________________________________________________________\n","100% 782/782 [01:19<00:00,  9.78it/s]\n","Test num: 25000 — Test loss: 0.3380905716085434 — Test accuracy: 87.58 — Time: 79.945601\n","________________________________________________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 20\n","Train num: 25000 — Train loss: 0.16354635912656784 — Time: 256.724783\n","100% 782/782 [01:20<00:00,  9.75it/s]\n","Val num: 25000 — Val loss: 0.4099909391272068 — Val accuracy: 86.6 — Time: 80.219283\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 21\n","Train num: 25000 — Train loss: 0.14511156635284425 — Time: 256.694986\n","100% 782/782 [01:20<00:00,  9.76it/s]\n","Val num: 25000 — Val loss: 0.40970367147922515 — Val accuracy: 86.29599999999999 — Time: 80.082799\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 22\n","Train num: 25000 — Train loss: 0.133367667170763 — Time: 256.437955\n","100% 782/782 [01:19<00:00,  9.84it/s]\n","Val num: 25000 — Val loss: 0.3785554158091545 — Val accuracy: 86.548 — Time: 79.4678\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 23\n","Train num: 25000 — Train loss: 0.12230264850616455 — Time: 256.227265\n","100% 782/782 [01:19<00:00,  9.80it/s]\n","Val num: 25000 — Val loss: 0.4636907464122772 — Val accuracy: 85.236 — Time: 79.777956\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 24\n","Train num: 25000 — Train loss: 0.12313488813757896 — Time: 256.752343\n","100% 782/782 [01:19<00:00,  9.78it/s]\n","Val num: 25000 — Val loss: 0.4075829392170906 — Val accuracy: 86.568 — Time: 79.954111\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 25\n","Train num: 25000 — Train loss: 0.11599711924433709 — Time: 256.763223\n","100% 782/782 [01:20<00:00,  9.72it/s]\n","Val num: 25000 — Val loss: 0.43838435392171143 — Val accuracy: 86.96000000000001 — Time: 80.481153\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 26\n","Train num: 25000 — Train loss: 0.09853990228712559 — Time: 256.756464\n","100% 782/782 [01:20<00:00,  9.75it/s]\n","Val num: 25000 — Val loss: 0.48532385444879533 — Val accuracy: 85.464 — Time: 80.189644\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 27\n","Train num: 25000 — Train loss: 0.09543191941171884 — Time: 256.73857\n","100% 782/782 [01:20<00:00,  9.76it/s]\n","Val num: 25000 — Val loss: 0.454762473269999 — Val accuracy: 86.644 — Time: 80.145823\n","________________________________________________________________________________\n","100% 782/782 [04:16<00:00,  3.05it/s]\n","Epoch: 28\n","Train num: 25000 — Train loss: 0.08026310792997479 — Time: 256.709445\n","100% 782/782 [01:20<00:00,  9.67it/s]\n","Val num: 25000 — Val loss: 0.6223272951197624 — Val accuracy: 85.22800000000001 — Time: 80.840407\n","________________________________________________________________________________\n","100% 782/782 [04:17<00:00,  3.04it/s]\n","Epoch: 29\n","Train num: 25000 — Train loss: 0.07504869109377264 — Time: 257.124213\n","100% 782/782 [01:20<00:00,  9.73it/s]\n","Val num: 25000 — Val loss: 0.48650347168684005 — Val accuracy: 86.568 — Time: 80.38885\n","________________________________________________________________________________\n","best test acc: 87.58\n","________________________________________________________________________________________________________________________________________________________________________________________________________\n"]}],"source":["# attention 30\n","!python3 \"lra_main.py\" --task text_4000 --model CDIL"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10750169,"status":"ok","timestamp":1682254205337,"user":{"displayName":"Li An","userId":"00679100983466103760"},"user_tz":420},"id":"KyH2KLYwMhuN","outputId":"1cca0a93-1b53-41a2-f8b2-00d9ea5eedd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["position: 2000\n","layer:1 \t distance:3 \t size:3\n","layer:2 \t distance:7 \t size:7\n","layer:3 \t distance:15 \t size:15\n","layer:4 \t distance:31 \t size:31\n","layer:5 \t distance:63 \t size:63\n","layer:6 \t distance:127 \t size:127\n","layer:7 \t distance:255 \t size:255\n","layer:8 \t distance:511 \t size:511\n","layer:9 \t distance:1023 \t size:1023\n","layer:10 \t distance:2047 \t size:2047\n","layer:11 \t distance:4000 \t size:4000\n","\n","CONV(\n","  (embedding): Embedding(256, 64)\n","  (positional_encoding): Embedding(4000, 64)\n","  (conv): ConvPart(\n","    (conv_net): Sequential(\n","      (0): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (1): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (2): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (3): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (4): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (5): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (6): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (7): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (8): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(256,), dilation=(256,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (9): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(512,), dilation=(512,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","      (10): Block(\n","        (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1024,), dilation=(1024,), padding_mode=circular)\n","        (nonlinear): ReLU()\n","      )\n","    )\n","  )\n","  (linear): Linear(in_features=64, out_features=2, bias=True)\n",")\n","Tesla T4\n","text_4000_P409090_CDIL_S1_L11_H64\n","Loaded ./lra_datasets/text_4000.train.pickle... size=25000\n","Loaded ./lra_datasets/text_4000.dev.pickle... size=25000\n","Loaded ./lra_datasets/text_4000.test.pickle... size=25000\n","100% 782/782 [01:31<00:00,  8.52it/s]\n","Epoch: 0\n","Train num: 25000 — Train loss: 0.7242407289123535 — Time: 91.750935\n","100% 782/782 [00:19<00:00, 39.78it/s]\n","Val num: 25000 — Val loss: 0.6909498156929016 — Val accuracy: 53.872 — Time: 19.659034\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.49it/s]\n","Test num: 25000 — Test loss: 0.69094981590271 — Test accuracy: 53.872 — Time: 19.803597\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.13it/s]\n","Epoch: 1\n","Train num: 25000 — Train loss: 0.6874601982688904 — Time: 85.663387\n","100% 782/782 [00:19<00:00, 39.80it/s]\n","Val num: 25000 — Val loss: 0.6920271780776978 — Val accuracy: 51.94800000000001 — Time: 19.650903\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 2\n","Train num: 25000 — Train loss: 0.6478637443542481 — Time: 85.574814\n","100% 782/782 [00:19<00:00, 39.49it/s]\n","Val num: 25000 — Val loss: 0.6589368233680725 — Val accuracy: 62.52799999999999 — Time: 19.803143\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.11it/s]\n","Test num: 25000 — Test loss: 0.6589368258666992 — Test accuracy: 62.52799999999999 — Time: 19.996748\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 3\n","Train num: 25000 — Train loss: 0.6139935215950012 — Time: 85.426773\n","100% 782/782 [00:19<00:00, 39.69it/s]\n","Val num: 25000 — Val loss: 0.5937653506088257 — Val accuracy: 68.732 — Time: 19.704263\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.63it/s]\n","Test num: 25000 — Test loss: 0.5937653512191773 — Test accuracy: 68.732 — Time: 19.732802\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 4\n","Train num: 25000 — Train loss: 0.5782537770652771 — Time: 85.198202\n","100% 782/782 [00:19<00:00, 39.67it/s]\n","Val num: 25000 — Val loss: 0.5799365580368042 — Val accuracy: 69.58800000000001 — Time: 19.710536\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.77it/s]\n","Test num: 25000 — Test loss: 0.5799365584373474 — Test accuracy: 69.58800000000001 — Time: 19.6659\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 5\n","Train num: 25000 — Train loss: 0.538148602771759 — Time: 85.301857\n","100% 782/782 [00:19<00:00, 39.54it/s]\n","Val num: 25000 — Val loss: 0.533653846206665 — Val accuracy: 74.0 — Time: 19.779645\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.73it/s]\n","Test num: 25000 — Test loss: 0.5336538450622559 — Test accuracy: 74.0 — Time: 19.68502\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 6\n","Train num: 25000 — Train loss: 0.49806170080184936 — Time: 85.21197\n","100% 782/782 [00:19<00:00, 39.69it/s]\n","Val num: 25000 — Val loss: 0.500775733909607 — Val accuracy: 75.996 — Time: 19.704277\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.58it/s]\n","Test num: 25000 — Test loss: 0.5007757379531861 — Test accuracy: 75.996 — Time: 19.757373\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 7\n","Train num: 25000 — Train loss: 0.4482663085174561 — Time: 85.1467\n","100% 782/782 [00:19<00:00, 39.84it/s]\n","Val num: 25000 — Val loss: 0.5087626894187928 — Val accuracy: 75.488 — Time: 19.629259\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 8\n","Train num: 25000 — Train loss: 0.41892635809898376 — Time: 85.098766\n","100% 782/782 [00:19<00:00, 39.78it/s]\n","Val num: 25000 — Val loss: 0.45740366376399993 — Val accuracy: 78.44 — Time: 19.659675\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.62it/s]\n","Test num: 25000 — Test loss: 0.4574036646461487 — Test accuracy: 78.44 — Time: 19.735894\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 9\n","Train num: 25000 — Train loss: 0.3850013088035584 — Time: 85.162652\n","100% 782/782 [00:19<00:00, 39.80it/s]\n","Val num: 25000 — Val loss: 0.4576268561553955 — Val accuracy: 78.56400000000001 — Time: 19.647005\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.75it/s]\n","Test num: 25000 — Test loss: 0.4576268561553955 — Test accuracy: 78.56400000000001 — Time: 19.673374\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 10\n","Train num: 25000 — Train loss: 0.3443044530677795 — Time: 85.163039\n","100% 782/782 [00:19<00:00, 39.17it/s]\n","Val num: 25000 — Val loss: 0.4601636772346497 — Val accuracy: 78.28399999999999 — Time: 19.96548\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 11\n","Train num: 25000 — Train loss: 0.3151084178519249 — Time: 85.206462\n","100% 782/782 [00:19<00:00, 39.83it/s]\n","Val num: 25000 — Val loss: 0.6160193480682373 — Val accuracy: 73.48400000000001 — Time: 19.633604\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 12\n","Train num: 25000 — Train loss: 0.28126641528606416 — Time: 85.164344\n","100% 782/782 [00:19<00:00, 39.56it/s]\n","Val num: 25000 — Val loss: 0.4922452632522583 — Val accuracy: 78.82400000000001 — Time: 19.768622\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.65it/s]\n","Test num: 25000 — Test loss: 0.49224526542663577 — Test accuracy: 78.82400000000001 — Time: 19.722533\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 13\n","Train num: 25000 — Train loss: 0.24778757974147797 — Time: 85.330412\n","100% 782/782 [00:19<00:00, 39.31it/s]\n","Val num: 25000 — Val loss: 0.5544598857116699 — Val accuracy: 78.468 — Time: 19.89342\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 14\n","Train num: 25000 — Train loss: 0.22175073538541795 — Time: 85.240336\n","100% 782/782 [00:19<00:00, 39.56it/s]\n","Val num: 25000 — Val loss: 0.5388541809272767 — Val accuracy: 78.68 — Time: 19.766898\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 15\n","Train num: 25000 — Train loss: 0.20703086000204085 — Time: 85.241191\n","100% 782/782 [00:19<00:00, 39.58it/s]\n","Val num: 25000 — Val loss: 0.585562082285881 — Val accuracy: 78.51599999999999 — Time: 19.758176\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 16\n","Train num: 25000 — Train loss: 0.1739875993514061 — Time: 85.272043\n","100% 782/782 [00:19<00:00, 39.75it/s]\n","Val num: 25000 — Val loss: 0.645140024023056 — Val accuracy: 78.908 — Time: 19.671414\n","________________________________________________________________________________\n","100% 782/782 [00:19<00:00, 39.53it/s]\n","Test num: 25000 — Test loss: 0.6451400236129761 — Test accuracy: 78.908 — Time: 19.780684\n","________________________________________________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 17\n","Train num: 25000 — Train loss: 0.14799653391480447 — Time: 85.106608\n","100% 782/782 [00:19<00:00, 39.59it/s]\n","Val num: 25000 — Val loss: 0.654895302772522 — Val accuracy: 78.884 — Time: 19.753881\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 18\n","Train num: 25000 — Train loss: 0.1258300202858448 — Time: 85.064261\n","100% 782/782 [00:19<00:00, 39.78it/s]\n","Val num: 25000 — Val loss: 0.9829757979393006 — Val accuracy: 75.884 — Time: 19.656741\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 19\n","Train num: 25000 — Train loss: 0.10708765614330769 — Time: 85.092758\n","100% 782/782 [00:19<00:00, 39.61it/s]\n","Val num: 25000 — Val loss: 1.004413436870575 — Val accuracy: 75.984 — Time: 19.741722\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 20\n","Train num: 25000 — Train loss: 0.08979403038561344 — Time: 85.086328\n","100% 782/782 [00:19<00:00, 39.66it/s]\n","Val num: 25000 — Val loss: 1.0527813945770264 — Val accuracy: 77.648 — Time: 19.719499\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 21\n","Train num: 25000 — Train loss: 0.08943882071509958 — Time: 85.249617\n","100% 782/782 [00:19<00:00, 39.63it/s]\n","Val num: 25000 — Val loss: 0.8976835958051681 — Val accuracy: 78.604 — Time: 19.731146\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 22\n","Train num: 25000 — Train loss: 0.06815528970964253 — Time: 85.318739\n","100% 782/782 [00:19<00:00, 39.65it/s]\n","Val num: 25000 — Val loss: 0.9558360207330435 — Val accuracy: 78.192 — Time: 19.72155\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 23\n","Train num: 25000 — Train loss: 0.05062662231735885 — Time: 85.188669\n","100% 782/782 [00:19<00:00, 39.30it/s]\n","Val num: 25000 — Val loss: 1.1835495756602288 — Val accuracy: 78.328 — Time: 19.900584\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 24\n","Train num: 25000 — Train loss: 0.07101198873408139 — Time: 85.237371\n","100% 782/782 [00:19<00:00, 39.72it/s]\n","Val num: 25000 — Val loss: 1.0343971335625648 — Val accuracy: 78.224 — Time: 19.689964\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 25\n","Train num: 25000 — Train loss: 0.050102013670336454 — Time: 85.245202\n","100% 782/782 [00:19<00:00, 39.52it/s]\n","Val num: 25000 — Val loss: 1.0927092302274704 — Val accuracy: 77.79599999999999 — Time: 19.786713\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 26\n","Train num: 25000 — Train loss: 0.051686669810265304 — Time: 85.314325\n","100% 782/782 [00:20<00:00, 39.04it/s]\n","Val num: 25000 — Val loss: 1.518083265209198 — Val accuracy: 74.59599999999999 — Time: 20.032998\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 27\n","Train num: 25000 — Train loss: 0.038653957488425074 — Time: 85.297287\n","100% 782/782 [00:19<00:00, 39.60it/s]\n","Val num: 25000 — Val loss: 1.5423911353683473 — Val accuracy: 71.48400000000001 — Time: 19.749389\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 28\n","Train num: 25000 — Train loss: 0.0422121264867764 — Time: 85.187925\n","100% 782/782 [00:19<00:00, 39.59it/s]\n","Val num: 25000 — Val loss: 1.2657075750851632 — Val accuracy: 78.172 — Time: 19.75486\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 29\n","Train num: 25000 — Train loss: 0.04673118678617291 — Time: 85.145968\n","100% 782/782 [00:19<00:00, 39.46it/s]\n","Val num: 25000 — Val loss: 1.6837215868282318 — Val accuracy: 73.72 — Time: 19.818048\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 30\n","Train num: 25000 — Train loss: 0.04710041707015596 — Time: 85.080378\n","100% 782/782 [00:19<00:00, 39.68it/s]\n","Val num: 25000 — Val loss: 1.203552811717987 — Val accuracy: 77.34 — Time: 19.71009\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 31\n","Train num: 25000 — Train loss: 0.030025303134433926 — Time: 85.223634\n","100% 782/782 [00:19<00:00, 39.56it/s]\n","Val num: 25000 — Val loss: 1.2121915523529052 — Val accuracy: 77.56 — Time: 19.765338\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 32\n","Train num: 25000 — Train loss: 0.03321557024696842 — Time: 85.1045\n","100% 782/782 [00:19<00:00, 39.51it/s]\n","Val num: 25000 — Val loss: 1.36030467993021 — Val accuracy: 77.968 — Time: 19.793259\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 33\n","Train num: 25000 — Train loss: 0.03455951737759635 — Time: 85.116311\n","100% 782/782 [00:19<00:00, 39.48it/s]\n","Val num: 25000 — Val loss: 1.4458737377929687 — Val accuracy: 76.464 — Time: 19.80916\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 34\n","Train num: 25000 — Train loss: 0.03227819009112194 — Time: 85.12163\n","100% 782/782 [00:19<00:00, 39.65it/s]\n","Val num: 25000 — Val loss: 1.2028389696788788 — Val accuracy: 77.8 — Time: 19.721968\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 35\n","Train num: 25000 — Train loss: 0.02836733463725075 — Time: 85.127903\n","100% 782/782 [00:19<00:00, 39.75it/s]\n","Val num: 25000 — Val loss: 1.3891776090240477 — Val accuracy: 77.544 — Time: 19.671902\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 36\n","Train num: 25000 — Train loss: 0.02677118330507772 — Time: 85.201467\n","100% 782/782 [00:19<00:00, 39.45it/s]\n","Val num: 25000 — Val loss: 1.3198584247922898 — Val accuracy: 78.104 — Time: 19.822047\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 37\n","Train num: 25000 — Train loss: 0.03000075856901705 — Time: 85.172585\n","100% 782/782 [00:19<00:00, 39.54it/s]\n","Val num: 25000 — Val loss: 1.3794449862289428 — Val accuracy: 77.94 — Time: 19.777059\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 38\n","Train num: 25000 — Train loss: 0.033473557240705706 — Time: 85.188692\n","100% 782/782 [00:19<00:00, 39.54it/s]\n","Val num: 25000 — Val loss: 1.0779527887690068 — Val accuracy: 77.888 — Time: 19.777227\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 39\n","Train num: 25000 — Train loss: 0.023831274233683943 — Time: 85.272637\n","100% 782/782 [00:19<00:00, 39.37it/s]\n","Val num: 25000 — Val loss: 1.3637520607498288 — Val accuracy: 77.428 — Time: 19.861688\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 40\n","Train num: 25000 — Train loss: 0.031232766269352286 — Time: 85.370325\n","100% 782/782 [00:20<00:00, 39.09it/s]\n","Val num: 25000 — Val loss: 1.4912924937915801 — Val accuracy: 76.292 — Time: 20.005682\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 41\n","Train num: 25000 — Train loss: 0.022442215621564537 — Time: 85.308865\n","100% 782/782 [00:19<00:00, 39.62it/s]\n","Val num: 25000 — Val loss: 1.5716235162639618 — Val accuracy: 77.944 — Time: 19.739669\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 42\n","Train num: 25000 — Train loss: 0.0366843158739619 — Time: 85.293566\n","100% 782/782 [00:19<00:00, 39.37it/s]\n","Val num: 25000 — Val loss: 2.143484126434326 — Val accuracy: 74.112 — Time: 19.861395\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 43\n","Train num: 25000 — Train loss: 0.019709058057018555 — Time: 85.354807\n","100% 782/782 [00:19<00:00, 39.34it/s]\n","Val num: 25000 — Val loss: 1.3324859542322158 — Val accuracy: 78.01599999999999 — Time: 19.880051\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 44\n","Train num: 25000 — Train loss: 0.024667932347143068 — Time: 85.277653\n","100% 782/782 [00:19<00:00, 39.43it/s]\n","Val num: 25000 — Val loss: 1.3558470679998398 — Val accuracy: 78.416 — Time: 19.834008\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 45\n","Train num: 25000 — Train loss: 0.015599819231291768 — Time: 85.195374\n","100% 782/782 [00:19<00:00, 39.63it/s]\n","Val num: 25000 — Val loss: 2.3707749098968507 — Val accuracy: 69.404 — Time: 19.73095\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 46\n","Train num: 25000 — Train loss: 0.02926261399276089 — Time: 85.290058\n","100% 782/782 [00:19<00:00, 39.31it/s]\n","Val num: 25000 — Val loss: 1.3101027919864654 — Val accuracy: 77.144 — Time: 19.89362\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.13it/s]\n","Epoch: 47\n","Train num: 25000 — Train loss: 0.02426335178941721 — Time: 85.639905\n","100% 782/782 [00:19<00:00, 39.17it/s]\n","Val num: 25000 — Val loss: 1.415273828315735 — Val accuracy: 78.16 — Time: 19.962452\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 48\n","Train num: 25000 — Train loss: 0.015133063649909455 — Time: 85.42151\n","100% 782/782 [00:19<00:00, 39.39it/s]\n","Val num: 25000 — Val loss: 1.3567098479771613 — Val accuracy: 78.18 — Time: 19.854954\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 49\n","Train num: 25000 — Train loss: 0.024428977534631267 — Time: 85.329864\n","100% 782/782 [00:20<00:00, 39.02it/s]\n","Val num: 25000 — Val loss: 1.3870729147911072 — Val accuracy: 77.84 — Time: 20.040077\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.13it/s]\n","Epoch: 50\n","Train num: 25000 — Train loss: 0.018367388250543737 — Time: 85.635226\n","100% 782/782 [00:20<00:00, 38.99it/s]\n","Val num: 25000 — Val loss: 1.7144955236458779 — Val accuracy: 75.364 — Time: 20.059304\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 51\n","Train num: 25000 — Train loss: 0.02434718071046751 — Time: 85.580602\n","100% 782/782 [00:20<00:00, 39.09it/s]\n","Val num: 25000 — Val loss: 1.52552776014328 — Val accuracy: 77.152 — Time: 20.006235\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.13it/s]\n","Epoch: 52\n","Train num: 25000 — Train loss: 0.014332470988878049 — Time: 85.654071\n","100% 782/782 [00:20<00:00, 39.00it/s]\n","Val num: 25000 — Val loss: 1.7241624567842484 — Val accuracy: 77.096 — Time: 20.049404\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 53\n","Train num: 25000 — Train loss: 0.018028992395973765 — Time: 85.542756\n","100% 782/782 [00:20<00:00, 38.98it/s]\n","Val num: 25000 — Val loss: 1.489329976644516 — Val accuracy: 77.836 — Time: 20.062867\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 54\n","Train num: 25000 — Train loss: 0.01568895927836478 — Time: 85.391122\n","100% 782/782 [00:20<00:00, 39.08it/s]\n","Val num: 25000 — Val loss: 1.4302646966171264 — Val accuracy: 78.044 — Time: 20.009649\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 55\n","Train num: 25000 — Train loss: 0.021448140958691946 — Time: 85.41822\n","100% 782/782 [00:19<00:00, 39.22it/s]\n","Val num: 25000 — Val loss: 1.5999049137806893 — Val accuracy: 78.348 — Time: 19.94054\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 56\n","Train num: 25000 — Train loss: 0.01962541895585484 — Time: 85.402158\n","100% 782/782 [00:19<00:00, 39.15it/s]\n","Val num: 25000 — Val loss: 1.3963084743225576 — Val accuracy: 77.608 — Time: 19.976289\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 57\n","Train num: 25000 — Train loss: 0.012024292726791464 — Time: 85.353077\n","100% 782/782 [00:20<00:00, 38.90it/s]\n","Val num: 25000 — Val loss: 2.1205648221898077 — Val accuracy: 76.756 — Time: 20.103833\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 58\n","Train num: 25000 — Train loss: 0.015473878816731448 — Time: 85.435453\n","100% 782/782 [00:20<00:00, 39.07it/s]\n","Val num: 25000 — Val loss: 1.5267367349934577 — Val accuracy: 78.29599999999999 — Time: 20.016019\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 59\n","Train num: 25000 — Train loss: 0.046468408904580864 — Time: 85.539509\n","100% 782/782 [00:19<00:00, 39.25it/s]\n","Val num: 25000 — Val loss: 1.3084144537210465 — Val accuracy: 77.4 — Time: 19.922975\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 60\n","Train num: 25000 — Train loss: 0.0014139167022806942 — Time: 85.584494\n","100% 782/782 [00:19<00:00, 39.16it/s]\n","Val num: 25000 — Val loss: 2.0156555467796324 — Val accuracy: 77.616 — Time: 19.970374\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 61\n","Train num: 25000 — Train loss: 0.01722476160711376 — Time: 85.500978\n","100% 782/782 [00:20<00:00, 39.02it/s]\n","Val num: 25000 — Val loss: 1.605518166370392 — Val accuracy: 78.22 — Time: 20.042864\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 62\n","Train num: 25000 — Train loss: 0.0003927404110156931 — Time: 85.484021\n","100% 782/782 [00:20<00:00, 39.02it/s]\n","Val num: 25000 — Val loss: 1.8192853237438202 — Val accuracy: 78.244 — Time: 20.039959\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 63\n","Train num: 25000 — Train loss: 8.949013615871081e-05 — Time: 85.418416\n","100% 782/782 [00:19<00:00, 39.11it/s]\n","Val num: 25000 — Val loss: 1.9188635931873321 — Val accuracy: 78.316 — Time: 19.994663\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 64\n","Train num: 25000 — Train loss: 4.06296083567031e-05 — Time: 85.499591\n","100% 782/782 [00:19<00:00, 39.25it/s]\n","Val num: 25000 — Val loss: 2.1337928064751623 — Val accuracy: 78.324 — Time: 19.922813\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 65\n","Train num: 25000 — Train loss: 1.4403867474093203e-05 — Time: 85.463554\n","100% 782/782 [00:20<00:00, 38.99it/s]\n","Val num: 25000 — Val loss: 2.3790418302631378 — Val accuracy: 78.508 — Time: 20.057432\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 66\n","Train num: 25000 — Train loss: 6.302467218236245e-06 — Time: 85.512408\n","100% 782/782 [00:19<00:00, 39.17it/s]\n","Val num: 25000 — Val loss: 2.5761021872711183 — Val accuracy: 78.496 — Time: 19.962657\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 67\n","Train num: 25000 — Train loss: 3.0904359111997338e-06 — Time: 85.437217\n","100% 782/782 [00:19<00:00, 39.27it/s]\n","Val num: 25000 — Val loss: 2.7797837416934965 — Val accuracy: 78.608 — Time: 19.916184\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 68\n","Train num: 25000 — Train loss: 1.6300278313772764e-06 — Time: 85.381054\n","100% 782/782 [00:19<00:00, 39.25it/s]\n","Val num: 25000 — Val loss: 2.9616404080200196 — Val accuracy: 78.608 — Time: 19.924632\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 69\n","Train num: 25000 — Train loss: 7.764121498419741e-07 — Time: 85.415755\n","100% 782/782 [00:20<00:00, 39.08it/s]\n","Val num: 25000 — Val loss: 3.204229424753338 — Val accuracy: 78.628 — Time: 20.009415\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 70\n","Train num: 25000 — Train loss: 3.6700779454491794e-07 — Time: 85.322511\n","100% 782/782 [00:19<00:00, 39.12it/s]\n","Val num: 25000 — Val loss: 3.3929710449865462 — Val accuracy: 78.608 — Time: 19.990355\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 71\n","Train num: 25000 — Train loss: 2.0081852229566265e-07 — Time: 85.400547\n","100% 782/782 [00:19<00:00, 39.22it/s]\n","Val num: 25000 — Val loss: 3.5980529304313658 — Val accuracy: 78.648 — Time: 19.940613\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 72\n","Train num: 25000 — Train loss: 1.0167074292212419e-07 — Time: 85.253593\n","100% 782/782 [00:19<00:00, 39.13it/s]\n","Val num: 25000 — Val loss: 3.8311234338903426 — Val accuracy: 78.648 — Time: 19.985621\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 73\n","Train num: 25000 — Train loss: 4.570950402637664e-08 — Time: 85.360045\n","100% 782/782 [00:20<00:00, 38.95it/s]\n","Val num: 25000 — Val loss: 4.065100607376099 — Val accuracy: 78.652 — Time: 20.076569\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 74\n","Train num: 25000 — Train loss: 2.109048663072599e-08 — Time: 85.366655\n","100% 782/782 [00:20<00:00, 38.95it/s]\n","Val num: 25000 — Val loss: 4.271511436235905 — Val accuracy: 78.632 — Time: 20.077264\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 75\n","Train num: 25000 — Train loss: 1.106738412232744e-08 — Time: 85.163483\n","100% 782/782 [00:20<00:00, 39.07it/s]\n","Val num: 25000 — Val loss: 4.442735120151043 — Val accuracy: 78.636 — Time: 20.014012\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 76\n","Train num: 25000 — Train loss: 6.165502429098524e-09 — Time: 85.315323\n","100% 782/782 [00:20<00:00, 39.09it/s]\n","Val num: 25000 — Val loss: 4.6218953922462465 — Val accuracy: 78.688 — Time: 20.007636\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 77\n","Train num: 25000 — Train loss: 3.51905739734093e-09 — Time: 85.319111\n","100% 782/782 [00:20<00:00, 39.09it/s]\n","Val num: 25000 — Val loss: 4.767543241996765 — Val accuracy: 78.64 — Time: 20.005281\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 78\n","Train num: 25000 — Train loss: 2.1553036452814922e-09 — Time: 85.208136\n","100% 782/782 [00:20<00:00, 38.88it/s]\n","Val num: 25000 — Val loss: 4.895768686561585 — Val accuracy: 78.66 — Time: 20.111135\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.16it/s]\n","Epoch: 79\n","Train num: 25000 — Train loss: 1.3399122792634443e-09 — Time: 85.345761\n","100% 782/782 [00:20<00:00, 38.83it/s]\n","Val num: 25000 — Val loss: 5.00989541361332 — Val accuracy: 78.68 — Time: 20.141534\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 80\n","Train num: 25000 — Train loss: 9.441374851348882e-10 — Time: 85.174229\n","100% 782/782 [00:20<00:00, 38.91it/s]\n","Val num: 25000 — Val loss: 5.106960978469848 — Val accuracy: 78.684 — Time: 20.095994\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 81\n","Train num: 25000 — Train loss: 5.531310642936659e-10 — Time: 85.051464\n","100% 782/782 [00:20<00:00, 38.98it/s]\n","Val num: 25000 — Val loss: 5.205458797028065 — Val accuracy: 78.712 — Time: 20.063507\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.17it/s]\n","Epoch: 82\n","Train num: 25000 — Train loss: 3.671645856684336e-10 — Time: 85.260831\n","100% 782/782 [00:20<00:00, 39.09it/s]\n","Val num: 25000 — Val loss: 5.28933528924942 — Val accuracy: 78.684 — Time: 20.002974\n","________________________________________________________________________________\n","100% 782/782 [01:24<00:00,  9.23it/s]\n","Epoch: 83\n","Train num: 25000 — Train loss: 1.9550322292616328e-10 — Time: 84.735322\n","100% 782/782 [00:19<00:00, 39.66it/s]\n","Val num: 25000 — Val loss: 5.371698448162078 — Val accuracy: 78.696 — Time: 19.718407\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 84\n","Train num: 25000 — Train loss: 8.106231206284065e-11 — Time: 85.164505\n","100% 782/782 [00:20<00:00, 38.92it/s]\n","Val num: 25000 — Val loss: 5.4461218098282815 — Val accuracy: 78.676 — Time: 20.092378\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 85\n","Train num: 25000 — Train loss: 1.9073485191256624e-11 — Time: 85.12842\n","100% 782/782 [00:20<00:00, 38.92it/s]\n","Val num: 25000 — Val loss: 5.5160926745224 — Val accuracy: 78.692 — Time: 20.090896\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 86\n","Train num: 25000 — Train loss: 1.4305113893442466e-11 — Time: 85.103469\n","100% 782/782 [00:20<00:00, 38.81it/s]\n","Val num: 25000 — Val loss: 5.62761656993689 — Val accuracy: 78.696 — Time: 20.149673\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.18it/s]\n","Epoch: 87\n","Train num: 25000 — Train loss: 9.536742595628312e-12 — Time: 85.192583\n","100% 782/782 [00:20<00:00, 38.96it/s]\n","Val num: 25000 — Val loss: 5.736276661434173 — Val accuracy: 78.724 — Time: 20.071261\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.19it/s]\n","Epoch: 88\n","Train num: 25000 — Train loss: 0.0 — Time: 85.102502\n","100% 782/782 [00:20<00:00, 39.02it/s]\n","Val num: 25000 — Val loss: 5.8660081994628905 — Val accuracy: 78.728 — Time: 20.043858\n","________________________________________________________________________________\n","100% 782/782 [01:24<00:00,  9.20it/s]\n","Epoch: 89\n","Train num: 25000 — Train loss: 0.0 — Time: 84.9866\n","100% 782/782 [00:20<00:00, 39.04it/s]\n","Val num: 25000 — Val loss: 6.0202508847045895 — Val accuracy: 78.828 — Time: 20.031521\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.20it/s]\n","Epoch: 90\n","Train num: 25000 — Train loss: 0.0 — Time: 85.046507\n","100% 782/782 [00:20<00:00, 38.86it/s]\n","Val num: 25000 — Val loss: 6.177693649339676 — Val accuracy: 78.732 — Time: 20.125853\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.20it/s]\n","Epoch: 91\n","Train num: 25000 — Train loss: 0.0 — Time: 85.020176\n","100% 782/782 [00:20<00:00, 38.74it/s]\n","Val num: 25000 — Val loss: 6.329554692001342 — Val accuracy: 78.788 — Time: 20.186007\n","________________________________________________________________________________\n","100% 782/782 [01:24<00:00,  9.20it/s]\n","Epoch: 92\n","Train num: 25000 — Train loss: 0.4158270248603817 — Time: 84.968517\n","100% 782/782 [00:20<00:00, 38.65it/s]\n","Val num: 25000 — Val loss: 0.7101061185741424 — Val accuracy: 74.368 — Time: 20.230673\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 93\n","Train num: 25000 — Train loss: 0.08064753009922802 — Time: 85.523248\n","100% 782/782 [00:20<00:00, 38.77it/s]\n","Val num: 25000 — Val loss: 1.1394215775203704 — Val accuracy: 78.204 — Time: 20.171583\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.13it/s]\n","Epoch: 94\n","Train num: 25000 — Train loss: 0.0034138145535963123 — Time: 85.63719\n","100% 782/782 [00:20<00:00, 38.98it/s]\n","Val num: 25000 — Val loss: 1.5005530353742838 — Val accuracy: 78.35600000000001 — Time: 20.06194\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 95\n","Train num: 25000 — Train loss: 0.0005569500225712545 — Time: 85.53729\n","100% 782/782 [00:20<00:00, 38.94it/s]\n","Val num: 25000 — Val loss: 1.6849184460496902 — Val accuracy: 78.416 — Time: 20.082821\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.13it/s]\n","Epoch: 96\n","Train num: 25000 — Train loss: 0.0002039286984976934 — Time: 85.620279\n","100% 782/782 [00:20<00:00, 38.93it/s]\n","Val num: 25000 — Val loss: 1.9090087021923066 — Val accuracy: 78.364 — Time: 20.085476\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.13it/s]\n","Epoch: 97\n","Train num: 25000 — Train loss: 8.727795630977198e-05 — Time: 85.643291\n","100% 782/782 [00:20<00:00, 38.83it/s]\n","Val num: 25000 — Val loss: 2.0725139709377287 — Val accuracy: 78.51599999999999 — Time: 20.139458\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.14it/s]\n","Epoch: 98\n","Train num: 25000 — Train loss: 4.398648081942156e-05 — Time: 85.596895\n","100% 782/782 [00:20<00:00, 38.75it/s]\n","Val num: 25000 — Val loss: 2.2280090626621245 — Val accuracy: 78.50399999999999 — Time: 20.181657\n","________________________________________________________________________________\n","100% 782/782 [01:25<00:00,  9.15it/s]\n","Epoch: 99\n","Train num: 25000 — Train loss: 2.372347832812011e-05 — Time: 85.439681\n","100% 782/782 [00:20<00:00, 38.88it/s]\n","Val num: 25000 — Val loss: 2.367566484053135 — Val accuracy: 78.648 — Time: 20.114071\n","________________________________________________________________________________\n","best test acc: 78.908\n","________________________________________________________________________________________________________________________________________________________________________________________________________\n"]}],"source":["!python3 \"lra_main.py\" --task text_4000 --model CDIL"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}